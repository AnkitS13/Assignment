{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "c:\\Users\\aanch\\Desktop\\lead_gen_chatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import bs4\n",
    "import json\n",
    "import re\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.messages import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_pages = [\n",
    "    \"https://nebula9.ai/services/gen-ai/\",\n",
    "    \"https://nebula9.ai/services/cloud-services/\",\n",
    "    \"https://nebula9.ai/services/artificial-intelligence-machine-learning/\",\n",
    "    \"https://nebula9.ai/services/reporting-and-analytics/\",\n",
    "    \"https://nebula9.ai/services/consulting-and-advisory/\",\n",
    "    \"https://nebula9.ai/services/product-management/\",\n",
    "    \"https://nebula9.ai/services/tech-engineering/\",\n",
    "    \"https://nebula9.ai/industries/\",\n",
    "    \"https://nebula9.ai/industries/education/\",\n",
    "    \"https://nebula9.ai/industries/publishing/\",\n",
    "    \"https://nebula9.ai/industries/sports-entertainment/\",\n",
    "    \"https://nebula9.ai/industries/healthcare/\",\n",
    "    \"https://nebula9.ai/industries/banking/\",\n",
    "    \"https://nebula9.ai/industries/insurance/\",\n",
    "    \"https://nebula9.ai/industries/financialservices/\",\n",
    "    \"https://nebula9.ai/industries/retail/\",\n",
    "    \"https://nebula9.ai/industries/travelairlines/\",\n",
    "    \"https://nebula9.ai/industries/manufacturing/\",\n",
    "    \"https://nebula9.ai/case-studies/\",\n",
    "    \"https://nebula9.ai/category/blog/\",\n",
    "    \"https://nebula9.ai/careers/\",\n",
    "    \"https://nebula9.ai/contact-us/\",\n",
    "    \"https://nebula9.ai/about-us/\",\n",
    "    \"https://nebula9.ai/contact-us/\",\n",
    "    \"https://nebula9.ai/approach/\",\n",
    "    \"https://nebula9.ai/engagement-model/\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM with the desired model\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents from the web pages\n",
    "loader = WebBaseLoader(web_paths=web_pages)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into chunks for better processing\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = []\n",
    "\n",
    "for doc in docs:\n",
    "    # Split the content into chunks\n",
    "    doc_chunks = text_splitter.split_documents([doc])\n",
    "    \n",
    "    # Assign a unique chunk_index for each chunk\n",
    "    for idx, chunk in enumerate(doc_chunks):\n",
    "        chunk.metadata[\"chunk_index\"] = idx\n",
    "        splits.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove tabs and newlines\n",
    "    text = text.replace(\"\\t\", \" \").replace(\"\\n\", \" \")\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a dictionary to store the content in the desired format\n",
    "scraped_data = []\n",
    "\n",
    "# Extract and format the scraped data\n",
    "for doc in splits:\n",
    "    cleaned_text = clean_text(doc.page_content)\n",
    "    scraped_data.append({\n",
    "        \"page_url\": doc.metadata.get(\"source\", \"Unknown\"), \n",
    "        \"title\": doc.metadata.get(\"title\", \"No Title Available\"),\n",
    "        \"text\": cleaned_text,                         \n",
    "        \"chunk_index\": doc.metadata.get(\"chunk_index\", 0),\n",
    "        \"word_count\": len(cleaned_text.split())\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aanch\\AppData\\Local\\Temp\\ipykernel_26072\\3326431551.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "# 3. Use Hugging Face embeddings \n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Initialize ChromaDB and store the embeddings\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"chroma_db\"  \n",
    ")\n",
    "\n",
    "# 5. Create a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 6. Persist the database to reuse it later\n",
    "#vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Contextualize questions with history-aware retriever\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Define QA chain\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use five sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# 8. Create final RAG chain\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: What are the services offered by Nebula9.ai ? \n",
      "A1: According to the provided context, Nebula9.ai offers the following services:\n",
      "\n",
      "1. Generative AI\n",
      "2. Artificial Intelligence & Machine Learning\n",
      "3. Tech Engineering\n",
      "4. Reporting and Analytics\n",
      "5. Cloud Services\n",
      "6. Product Management\n",
      "7. Consulting and Advisory\n"
     ]
    }
   ],
   "source": [
    "# 9. Maintain chat history and ask questions\n",
    "chat_history = []\n",
    "\n",
    "# First question\n",
    "question_1 = \"What are the services offered by Nebula9.ai ? \"\n",
    "response_1 = rag_chain.invoke({\"input\": question_1, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question_1),\n",
    "        AIMessage(content=response_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "print(\"Q1:\", question_1)\n",
    "print(\"A1:\", response_1[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2: What are cloud solutions services ?\n",
      "A2: According to the provided context, Nebula9.ai's cloud solutions services include:\n",
      "\n",
      "1. Tailored Cloud Solutions: Custom-built solutions that align with specific business requirements.\n",
      "2. Built for Growth: Adaptable cloud services designed for scalability, ensuring operations grow alongside the business.\n",
      "3. Uncompromised Security: Implementing cutting-edge security measures to protect against breaches and unauthorized interventions.\n",
      "4. Value-Driven Efficiency: Cloud strategies designed to deliver maximum ROI, optimizing costs and offering unparalleled value on every investment.\n"
     ]
    }
   ],
   "source": [
    "# Follow-up question\n",
    "question_2 = \"What are cloud solutions services ?\"\n",
    "response_2 = rag_chain.invoke({\"input\": question_2, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question_2),\n",
    "        AIMessage(content=response_2[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "print(\"Q2:\", question_2)\n",
    "print(\"A2:\", response_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
